{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1430623,"sourceType":"datasetVersion","datasetId":609422},{"sourceId":3144322,"sourceType":"datasetVersion","datasetId":1887646},{"sourceId":3799317,"sourceType":"datasetVersion","datasetId":2265176},{"sourceId":7510067,"sourceType":"datasetVersion","datasetId":767979}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim.lr_scheduler import StepLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom transformers import AutoConfig, CvtForImageClassification\nimport numpy as np\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom PIL import Image, UnidentifiedImageError\nimport os\nfrom sklearn.metrics import confusion_matrix, f1_score, classification_report\nimport matplotlib.pyplot as plt\nfrom torch.utils.tensorboard import SummaryWriter\nimport seaborn as sns\nimport random\nfrom collections import Counter\nimport optuna\nfrom optuna import Trial\nfrom optuna.samplers import TPESampler\nfrom tqdm import tqdm\nimport logging\n\n# ----------------------------\n# 1. Konfigurasi Logging\n# ----------------------------\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s',\n    handlers=[\n        logging.FileHandler(\"optimization.log\"),\n        logging.StreamHandler()\n    ]\n)\nlogger = logging.getLogger()\n\n\n\n","metadata":{"id":"iC1-ZNmCw15s","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 2. Set seed for reproducibility\n# ----------------------------\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nBATCH_SIZE = 16\nNUM_CLASSES = 20\nEPOCHS = 100\nIMAGE_SIZE = 224\nDATASET_PATH = '/kaggle/input/d/phiard/aksara-jawa/v3/v3'  # Pastikan path ini benar","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 3. Transformasi Data\n# ----------------------------\ntrain_transform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.CenterCrop(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\nval_transform = transforms.Compose([\n    transforms.Resize(IMAGE_SIZE),\n    transforms.CenterCrop(IMAGE_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 4. Custom Dataset\n# ----------------------------\nclass CustomImageFolder(Dataset):\n    def __init__(self, root, transform=None):\n        self.samples = []\n        self.targets = []\n        self.transform = transform\n        self.classes, self.class_to_idx = self._find_classes(root)\n        self.num_classes = len(self.classes)\n\n        for target_class in sorted(self.class_to_idx.keys()):\n            class_index = self.class_to_idx[target_class]\n            target_dir = os.path.join(root, target_class)\n            if not os.path.isdir(target_dir):\n                continue\n            for root_, _, fnames in sorted(os.walk(target_dir, followlinks=True)):\n                for fname in sorted(fnames):\n                    path = os.path.join(root_, fname)\n                    try:\n                        with Image.open(path) as img:\n                            img.verify()  # Verifikasi integritas gambar\n                        self.samples.append((path, class_index))\n                        self.targets.append(class_index)\n                    except (UnidentifiedImageError, OSError):\n                        logger.warning(f\"Gambar korup dilewati: {path}\")\n                        continue\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        with Image.open(path) as sample:\n            sample = sample.convert('RGB')\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample, target\n\n    def __len__(self):\n        return len(self.samples)\n\n    def _find_classes(self, dir):\n        classes = [d.name for d in os.scandir(dir) if d.is_dir()]\n        classes.sort()\n        class_to_idx = {classes[i]: i for i in range(len(classes))}\n        return classes, class_to_idx\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 5. Membuat DataLoader\n# ----------------------------\ntrain_dir = os.path.join(DATASET_PATH, 'train')\nval_dir = os.path.join(DATASET_PATH, 'val')\n\ntrain_dataset = CustomImageFolder(train_dir, transform=train_transform)\nval_dataset = CustomImageFolder(val_dir, transform=val_transform)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 6. Verifikasi Distribusi Kelas\n# ----------------------------\ncounter = Counter(train_dataset.targets)\nlogger.info(\"Distribusi Kelas dalam Training Set:\")\nfor cls, count in counter.items():\n    logger.info(f\"Kelas {cls}: {count} sampel\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 7. Menghitung Class Weights\n# ----------------------------\nclass_weights = compute_class_weight(\n    class_weight='balanced',\n    classes=np.unique(train_dataset.targets),\n    y=train_dataset.targets\n)\nclass_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\nlogger.info(f\"Class Weights: {class_weights}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 8. Fungsi untuk Melatih Satu Epoch\n# ----------------------------\ndef train_epoch(model, loader, optimizer, criterion):\n    model.train()\n    total_loss = 0\n    correct = 0\n    total = 0\n    for data, target in loader:\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(data).logits\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n        _, predicted = output.max(1)\n        correct += predicted.eq(target).sum().item()\n        total += target.size(0)\n    return total_loss / len(loader), 100. * correct / total\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ----------------------------\n# 9. Fungsi untuk Validasi\n# ----------------------------\ndef validate(model, loader, criterion):\n    model.eval()\n    total_loss = 0\n    correct = 0\n    total = 0\n    all_targets = []\n    all_predictions = []\n    with torch.no_grad():\n        for data, target in loader:\n            data, target = data.to(device), target.to(device)\n            output = model(data).logits\n            loss = criterion(output, target)\n            total_loss += loss.item()\n            _, predicted = output.max(1)\n            correct += predicted.eq(target).sum().item()\n            total += target.size(0)\n            all_targets.extend(target.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n    f1 = f1_score(all_targets, all_predictions, average='weighted')\n    return total_loss / len(loader), 100. * correct / total, f1, all_targets, all_predictions\n\n   \n    \n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 10. Definisikan Fungsi Objektif untuk Optuna\n# ----------------------------\ndef objective(trial: Trial):\n    # Saran hyperparameter\n    lr = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-2)\n\n    logger.info(f\"Trial {trial.number}: Suggesting learning_rate={lr}, weight_decay={weight_decay}\")\n\n    # Konfigurasi model\n    config = AutoConfig.from_pretrained(\"microsoft/cvt-13\")\n    config.num_labels = NUM_CLASSES\n    config.hidden_dropout_prob = 0.1  # Mengurangi dropout\n\n    model = CvtForImageClassification.from_pretrained(\n        \"microsoft/cvt-13\", config=config, ignore_mismatched_sizes=True\n    ).to(device)\n\n    # Pastikan semua parameter dioptimalkan\n    for param in model.parameters():\n        param.requires_grad = True\n\n    # Definisikan loss function tanpa label smoothing\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.0)\n\n    # Definisikan optimizer dengan hyperparameter yang disarankan\n    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    # Definisikan scheduler\n    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n\n    # Variabel untuk tracking\n    best_val_acc = 0\n    counter = 0\n    patience = 20  # Meningkatkan patience untuk early stopping\n\n    # Loop Pelatihan dengan Progress Bar\n    for epoch in range(EPOCHS):\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n        val_loss, val_acc, val_f1, _, _ = validate(model, val_loader, criterion)\n\n        # Update scheduler\n        scheduler.step()\n\n        # Early Stopping\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            counter = 0\n        else:\n            counter += 1\n\n        # Melaporkan hasil ke Optuna\n        trial.report(val_acc, epoch)\n\n        # Memeriksa apakah trial harus dipangkas\n        if trial.should_prune():\n            logger.info(f\"Trial {trial.number} pruned at epoch {epoch+1}\")\n            raise optuna.exceptions.TrialPruned()\n\n        if counter >= patience:\n            logger.info(f\"Trial {trial.number} stopped early at epoch {epoch+1} due to no improvement.\")\n            break\n\n    logger.info(f\"Trial {trial.number} finished with best val_acc={best_val_acc}\")\n    return best_val_acc\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# ----------------------------\n# 11. Menambahkan Progress Bar untuk Optuna\n# ----------------------------\nclass TqdmOptunaCallback:\n    def __init__(self, total_trials):\n        self.pbar = tqdm(total=total_trials, desc=\"Optuna Optimization\", unit=\"trial\")\n\n    def __call__(self, study, trial):\n        if trial.state == optuna.trial.TrialState.COMPLETE:\n            self.pbar.update(1)\n        elif trial.state == optuna.trial.TrialState.PRUNED:\n            self.pbar.update(1)\n\n    def close(self):\n        self.pbar.close()\n\n# ----------------------------\n# 12. Membuat dan Menjalankan Studi Optuna\n# ----------------------------\ndef run_optuna_optimization():\n    sampler = TPESampler(seed=seed)\n    study = optuna.create_study(direction='maximize', sampler=sampler)\n    \n    # Tentukan jumlah trial\n    n_trials = 50\n\n    # Inisialisasi callback dengan progress bar\n    tqdm_callback = TqdmOptunaCallback(total_trials=n_trials)\n\n    try:\n        study.optimize(objective, n_trials=n_trials, callbacks=[tqdm_callback])\n    except KeyboardInterrupt:\n        logger.info(\"Optimization interrupted by user.\")\n\n    tqdm_callback.close()\n\n    logger.info(\"Optimization selesai.\")\n    logger.info(f\"Number of finished trials: {len(study.trials)}\")\n    logger.info(\"Best trial:\")\n    trial = study.best_trial\n\n    logger.info(f\"  Value: {trial.value}\")\n    logger.info(\"  Params: \")\n    for key, value in trial.params.items():\n        logger.info(f\"    {key}: {value}\")\n\n    # Menampilkan hasil visualisasi (Opsional)\n    try:\n        import optuna.visualization as vis\n        fig1 = vis.plot_optimization_history(study)\n        fig2 = vis.plot_param_importances(study)\n        fig1.show()\n        fig2.show()\n    except ImportError:\n        logger.warning(\"Optuna visualization modules not installed.\")\n\n    return study\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n# 13. Menjalankan Optimisasi\n# ----------------------------\nif __name__ == \"__main__\":\n    study = run_optuna_optimization()\n\n    # ----------------------------\n    # 14. Melatih Ulang dengan Hyperparameter Terbaik (Opsional)\n    # ----------------------------\n    best_lr = study.best_params['learning_rate']\n    best_weight_decay = study.best_params['weight_decay']\n    logger.info(f\"Best hyperparameters found: learning_rate={best_lr}, weight_decay={best_weight_decay}\")\n\n    # Inisialisasi model lagi dengan hyperparameter terbaik\n    config = AutoConfig.from_pretrained(\"microsoft/cvt-13\")\n    config.num_labels = NUM_CLASSES\n    config.hidden_dropout_prob = 0.1\n\n    model = CvtForImageClassification.from_pretrained(\n        \"microsoft/cvt-13\", config=config, ignore_mismatched_sizes=True\n    ).to(device)\n\n    for param in model.parameters():\n        param.requires_grad = True\n\n    # Definisikan loss function\n    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.0)\n\n    # Definisikan optimizer dengan hyperparameter terbaik\n    optimizer = optim.AdamW(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n\n    # Definisikan scheduler\n    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n\n    # Definisikan writer TensorBoard\n    writer = SummaryWriter()\n\n    # Variabel untuk tracking\n    best_val_acc = 0\n    counter = 0\n    patience = 20  # Meningkatkan patience untuk early stopping\n\n    # Loop Pelatihan dengan Progress Bar\n    for epoch in range(EPOCHS):\n        epoch_pbar = tqdm(total=1, desc=f\"Training Epoch {epoch+1}/{EPOCHS}\", unit=\"epoch\")\n        train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion)\n        val_loss, val_acc, val_f1, all_targets, all_predictions = validate(model, val_loader, criterion)\n\n        # Logging ke TensorBoard\n        writer.add_scalars('Loss', {'Train': train_loss, 'Val': val_loss}, epoch)\n        writer.add_scalars('Accuracy', {'Train': train_acc, 'Val': val_acc}, epoch)\n        writer.add_scalar('F1_Score/Val', val_f1, epoch)\n\n        # Simpan model terbaik\n        if val_acc > best_val_acc:\n            best_val_acc = val_acc\n            torch.save(model.state_dict(), \"best_cvt_model.pth\")\n            counter = 0  # Reset counter jika ada peningkatan\n        else:\n            counter += 1\n\n        # Early stopping\n        if counter >= patience:\n            logger.info(f\"Early stopping setelah epoch {epoch+1} karena tidak ada peningkatan akurasi validasi.\")\n            break\n\n        # Menampilkan progres\n        logger.info(f\"Epoch {epoch+1}/{EPOCHS}\")\n        logger.info(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n        logger.info(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%, Val F1 Score: {val_f1:.4f}\")\n\n        # Update scheduler\n        scheduler.step()\n\n        epoch_pbar.update(1)\n        epoch_pbar.close()\n\n    writer.close()\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # ----------------------------\n    # 15. Plot Confusion Matrix untuk Validasi\n    # ----------------------------\n    def plot_confusion_matrix(targets, predictions, class_names):\n        cm = confusion_matrix(targets, predictions)\n        plt.figure(figsize=(12, 10))\n        sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names)\n        plt.xlabel(\"Prediksi\")\n        plt.ylabel(\"Sebenarnya\")\n        plt.title(\"Confusion Matrix\")\n        plt.show()\n\n    plot_confusion_matrix(all_targets, all_predictions, train_dataset.classes)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n    # 16. Tampilkan Prediksi per Kelas\n    # ----------------------------\n    def show_predictions_per_class(model, dataset, num_classes=20, cols=5):\n        model.eval()\n        rows = (num_classes + cols - 1) // cols\n        fig, axs = plt.subplots(rows, cols, figsize=(15, 3 * rows))\n        axs = axs.flatten()\n\n        class_images = {i: None for i in range(num_classes)}\n\n        for i in range(len(dataset)):\n            image, label = dataset[i]\n            image = image.unsqueeze(0).to(device)\n            with torch.no_grad():\n                output = model(image).logits\n                _, predicted = output.max(1)\n                if class_images[label] is None and predicted.item() == label:\n                    class_images[label] = (image.squeeze().cpu(), label, predicted.item())\n                    if all(value is not None for value in class_images.values()):\n                        break\n\n        for i in range(num_classes):\n            ax = axs[i]\n            if class_images[i] is not None:\n                img, true_label, pred_label = class_images[i]\n                # Denormalisasi gambar\n                img = img * torch.tensor([0.229, 0.224, 0.225]).view(3,1,1) + torch.tensor([0.485, 0.456, 0.406]).view(3,1,1)\n                img = img.permute(1, 2, 0).numpy()\n                img = np.clip(img, 0, 1)\n                ax.imshow(img)\n                ax.set_title(f\"True: {train_dataset.classes[true_label]}\\nPred: {train_dataset.classes[pred_label]}\")\n                ax.axis('off')\n            else:\n                ax.text(0.5, 0.5, 'No Prediction', ha='center', va='center')\n                ax.axis('off')\n\n        for j in range(num_classes, len(axs)):\n            axs[j].axis('off')\n\n        plt.tight_layout()\n        plt.show()\n\n    show_predictions_per_class(model, val_dataset)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ----------------------------\n    # 17. Plot Loss dan Akurasi\n    # ----------------------------\n    plt.figure(figsize=(12, 5))\n\n    # Plot Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n\n    # Plot Akurasi\n    plt.subplot(1, 2, 2)\n    plt.plot(range(1, len(train_accs) + 1), train_accs, label='Train Accuracy')\n    plt.plot(range(1, len(val_accs) + 1), val_accs, label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    # ----------------------------\n  ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"  # 18. Evaluasi Tambahan: Classification Report\n    # ----------------------------\n    def classification_metrics(loader):\n        model.eval()\n        all_targets = []\n        all_predictions = []\n        with torch.no_grad():\n            for data, target in loader:\n                data, target = data.to(device), target.to(device)\n                output = model(data).logits\n                _, predicted = torch.max(output, 1)\n                all_targets.extend(target.cpu().numpy())\n                all_predictions.extend(predicted.cpu().numpy())\n        logger.info(classification_report(all_targets, all_predictions, target_names=train_dataset.classes))\n\n    logger.info(\"Classification Report untuk Training Set:\")\n    classification_metrics(train_loader)\n\n    logger.info(\"Classification Report untuk Validation Set:\")\n    classification_metrics(val_loader)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}